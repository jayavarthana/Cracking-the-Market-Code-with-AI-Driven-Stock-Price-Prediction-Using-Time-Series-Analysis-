{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi9hXDVLUt5/ETvsQ+yjG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayavarthana/Cracking-the-Market-Code-with-AI-Driven-Stock-Price-Prediction-Using-Time-Series-Analysis-/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Uz6Y2fANPQ",
        "outputId": "42575f3d-cb5e-4d49-951c-db86c1408640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Shape: (6, 3)\n",
            "                                               title  \\\n",
            "0  Donald Trump Sends Out Embarrassing New Yearâ€™s...   \n",
            "1  Watch The Exact Moment Paul Ryan Commits Caree...   \n",
            "2               NASA Finds Evidence of Water on Mars   \n",
            "3  Apple unveils new iPhone 15 with cutting-edge ...   \n",
            "4  BREAKING: Hillary Clinton Caught Using Illegal...   \n",
            "\n",
            "                                                text label  \n",
            "0  President Donald Trump sent out a Happy New Ye...  FAKE  \n",
            "1  Speaker Paul Ryan just committed career suicid...  FAKE  \n",
            "2  NASA has found the strongest evidence yet that...  REAL  \n",
            "3  Apple announced its newest iPhone with upgrade...  REAL  \n",
            "4  Hillary Clinton is under investigation again a...  FAKE  \n",
            "\n",
            "Accuracy: 0.00%\n",
            "Confusion Matrix:\n",
            "[[0 0]\n",
            " [2 0]]\n",
            "\n",
            "Prediction for custom news: FAKE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 1. Load the dataset\n",
        "df = pd.read_csv('fake_or_real_news.csv')  # Make sure to use your dataset path\n",
        "print(\"Dataset loaded. Shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# 2. Preprocess the text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+', '', text)  # Remove links\n",
        "    text = re.sub(r'@\\w+', '', text)          # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)          # Remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text)           # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# 3. Split the dataset\n",
        "X = df['text']\n",
        "y = df['label']  # Ensure 'label' column contains 'FAKE' and 'REAL'\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "# 4. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = vectorizer.fit_transform(X_train)\n",
        "tfidf_test = vectorizer.transform(X_test)\n",
        "\n",
        "# 5. Model training\n",
        "model = PassiveAggressiveClassifier(max_iter=50)\n",
        "model.fit(tfidf_train, y_train)\n",
        "\n",
        "# 6. Predictions and evaluation\n",
        "y_pred = model.predict(tfidf_test)\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy: {score*100:.2f}%\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# 7. Test on custom news\n",
        "def predict_news(news_text):\n",
        "    cleaned = clean_text(news_text)\n",
        "    vectorized = vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(vectorized)\n",
        "    return prediction[0]\n",
        "\n",
        "# Example\n",
        "sample_news = \"The government just announced a new policy to support farmers.\"\n",
        "print(\"\\nPrediction for custom news:\", predict_news(sample_news))\n"
      ]
    }
  ]
}